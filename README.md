<img width="1019" height="507" alt="Captura de pantalla 2025-11-25 140505" src="https://github.com/user-attachments/assets/f633f518-8bc1-40a5-99f0-60a9cb6718c4" />

# ğŸš€ Pro Scraper - Scraper Web Profesional

Este es un proyecto de web scraping avanzado y profesional diseÃ±ado para extraer datos del sitio web de prÃ¡ctica `books.toscrape.com`. El scraper navega a travÃ©s del catÃ¡logo de libros, accede a la pÃ¡gina de detalles de cada uno para extraer informaciÃ³n especÃ­fica (tÃ­tulo, precio y stock) y finalmente guarda los datos recopilados en un archivo CSV.

El proyecto estÃ¡ construido con un enfoque en la robustez, el respeto por el servidor, la mantenibilidad y una experiencia de usuario amigable en la consola.

## âœ¨ CaracterÃ­sticas Principales

- **Deep Crawling**: Navega por la paginaciÃ³n del catÃ¡logo y entra en la pÃ¡gina de detalle de cada producto para una extracciÃ³n profunda.
- **ExtracciÃ³n de Datos Detallada**: Extrae el tÃ­tulo, precio y la cantidad exacta de stock disponible de cada libro.
- **Manejo de Errores y Reintentos**: Implementa una lÃ³gica de reintentos configurable para manejar fallos en las solicitudes HTTP y aumentar la fiabilidad.
- **RotaciÃ³n de User-Agents**: Utiliza `fake-useragent` para rotar los User-Agents en cada solicitud, simulando trÃ¡fico desde diferentes navegadores y reduciendo la probabilidad de bloqueo.
- **Respeto al Servidor (Politeness)**: Incorpora retrasos aleatorios entre solicitudes para no sobrecargar el servidor de destino.
- **Logging Profesional**: Registra eventos importantes, advertencias y errores tanto en un archivo (`logs/scraper.log`) como en la consola, utilizando `rich` para un formato enriquecido y legible.
- **ExportaciÃ³n de Datos**: Guarda todos los datos extraÃ­dos en un archivo `data/processed/books_data.csv` para un fÃ¡cil anÃ¡lisis y uso posterior.
- **Interfaz de Consola Atractiva**: Emplea la librerÃ­a `rich` para mostrar barras de progreso, tablas de resumen y logs coloreados, mejorando la experiencia de ejecuciÃ³n.
- **ConfiguraciÃ³n Centralizada**: Permite modificar fÃ¡cilmente parÃ¡metros clave como la URL de destino, los reintentos y los rangos de espera a travÃ©s del archivo `config/settings.py`.

## ğŸ“‚ Estructura del Proyecto

```
Pro_scraper/
â”‚
â”œâ”€â”€ .env                  # (Opcional) Para variables de entorno
â”œâ”€â”€ .gitignore            # Archivos ignorados por Git
â”œâ”€â”€ main.py               # Punto de entrada para ejecutar el scraper
â”œâ”€â”€ requeriments.txt      # Dependencias del proyecto
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py       # Configuraciones principales del scraper
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ books_data.csv  # Archivo de salida con los datos
â”‚   â””â”€â”€ raw/              # (Opcional) Para guardar HTML crudo
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ scraper.log       # Archivo de log
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ scraper.py        # Clase principal con la lÃ³gica del scraper
    â””â”€â”€ utils.py          # Funciones de utilidad (limpieza de datos, etc.)
```

## ğŸ› ï¸ InstalaciÃ³n

Sigue estos pasos para configurar el entorno y ejecutar el scraper.

**1. Clonar el Repositorio (Opcional)**

Si estÃ¡s trabajando desde una copia local, puedes omitir este paso.
```bash
git clone <URL-DEL-REPOSITORIO>
cd Pro_scraper
```

**2. Crear un Entorno Virtual**

Es una buena prÃ¡ctica aislar las dependencias del proyecto.
```bash
python -m venv venv
```
Y activarlo:
- En Windows:
  ```bash
  .\venv\Scripts\activate
  ```
- En macOS/Linux:
  ```bash
  source venv/bin/activate
  ```

**3. Instalar Dependencias**

Instala todas las librerÃ­as necesarias desde el archivo `requeriments.txt`.
```bash
pip install -r requeriments.txt
pip install rich # AsegÃºrate de tener rich tambiÃ©n
```

## â–¶ï¸ Uso

Para ejecutar el scraper, simplemente corre el archivo `main.py` desde la raÃ­z del proyecto:

```bash
python main.py
```

El script comenzarÃ¡ a procesar las pÃ¡ginas, mostrando el progreso en la consola. Al finalizar, encontrarÃ¡s los datos en `data/processed/books_data.csv` y un registro detallado en `logs/scraper.log`.

## âš™ï¸ ConfiguraciÃ³n

Puedes personalizar el comportamiento del scraper modificando el archivo `config/settings.py`.

- `BASE_URL`: La URL del sitio a scrapear.
- `OUTPUT_FILE`: La ruta del archivo CSV de salida.
- `LOG_FILE`: La ruta del archivo de log.
- `MAX_RETRIES`: NÃºmero mÃ¡ximo de reintentos por solicitud.
- `DELAY_RANGE`: Tupla que define el rango (mÃ­nimo, mÃ¡ximo) de segundos de espera entre solicitudes.

## ğŸ“š Dependencias

- **requests**: Para realizar las solicitudes HTTP.
- **beautifulsoup4**: Para parsear el contenido HTML.
- **pandas**: Para estructurar y guardar los datos en formato CSV.
- **fake-useragent**: Para generar User-Agents aleatorios.
- **rich**: Para crear interfaces de lÃ­nea de comandos atractivas.
- **python-dotenv**: Para gestionar variables de entorno (opcional).
